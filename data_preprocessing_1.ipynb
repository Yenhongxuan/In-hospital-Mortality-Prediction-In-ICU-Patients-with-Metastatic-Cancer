{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableone import TableOne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ydata_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute the vitalsign data and baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw_data/vitalsign.csv') # Read the vitalsign data\n",
    "# columns to impute\n",
    "imputed_columns = ['label_hosp', 'label_icu', 'icu_timestep', 'icu_timestep_back', 'hosp_timestep_back', 'Heart_rate', 'NI_SBP', 'NI_DBP', 'NI_MBP', 'Glucose', 'tempture_C', 'sofa', 'kdigo_creat']\n",
    "# columns related to urineoutput\n",
    "urine_columns = ['urineoutput_6hr', 'urineoutput_12hr', 'urineoutput_24hr']\n",
    "all_columns = ['stay_id'] + imputed_columns  + urine_columns\n",
    "# Impute columns by last observation first, impute by next observation senondly\n",
    "for col in imputed_columns:\n",
    "    df[col] = df.groupby('stay_id')[col].fillna(method='ffill')\n",
    "    df[col] = df.groupby('stay_id')[col].fillna(method='bfill')\n",
    "# for urineoutput, we could only use the last observation. We filling with 0 instead of imputing by next observation\n",
    "for col in urine_columns:\n",
    "    df[col] = df.groupby('stay_id')[col].fillna(method='ffill')\n",
    "    df[col] = df.groupby('stay_id')[col].fillna(0)\n",
    "df = df[all_columns]\n",
    "\n",
    "# Remov the stay_id with Nan value after previous impute\n",
    "ids_with_null_total = set() \n",
    "sumup = {}\n",
    "for col in all_columns:\n",
    "    rows_with_null = df[df[['stay_id', col]].isnull().any(axis=1)]\n",
    "    ids_with_null = set(rows_with_null['stay_id'].tolist())\n",
    "    ids_with_null_total.update(rows_with_null['stay_id'].tolist())\n",
    "    sumup[col] = len(ids_with_null)\n",
    "\n",
    "# for k, v in sumup.items():\n",
    "#     print(f'{k}: {v}')\n",
    "print(len(ids_with_null_total))\n",
    "df = df[~df['stay_id'].isin(ids_with_null_total)]\n",
    "\n",
    "# proces the part of urineoutput. divide by the weight of patient and the hours\n",
    "df_b = pd.read_csv('./raw_data/baseline3.csv') # baseline that contain the weight of patient\n",
    "df_weight  = df_b[['stay_id', 'weight']]\n",
    "df = pd.merge(df, df_weight, how='inner', on='stay_id')\n",
    "# normalize the urineoutput\n",
    "df['urineoutput_6hr'] = df['urineoutput_6hr'] / df['weight'] / 6\n",
    "df['urineoutput_12hr'] = df['urineoutput_12hr'] / df['weight'] / 12\n",
    "df['urineoutput_24hr'] = df['urineoutput_24hr'] / df['weight'] / 24\n",
    "df = df.drop('weight', axis=1)\n",
    "df.to_csv('data/vitalsign_processed.csv')\n",
    "print(len(df['stay_id'].unique())) \n",
    "\n",
    "\n",
    "# baseline part\n",
    "df = pd.read_csv('./raw_data/baseline.csv') # level 1 and 2 baseline data\n",
    "df_3 = pd.read_csv('./raw_data/baseline3.csv') # label 3 baseline data\n",
    "df_all = pd.merge(df, df_3, how='inner', on='stay_id')\n",
    "not_columns = [col for col in df_all.columns if any(col.startswith(prefix) for prefix in ['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime', 'dischtime', 'dod', 'label_icu_', 'label_hosp_', 'icu_timestep_back_', 'hosp_timestep_back_', 'ectopy_type', 'ectopy_frequency', 'ectopy_type_secondary', 'ectopy_frequency_secondary'])]\n",
    "columns = ['stay_id'] + [col for col in df_all.columns if col not in not_columns]\n",
    "df_all = df_all[columns]\n",
    "df_all = df_all[~df_all['stay_id'].isin(ids_with_null_total)] # align the stay_id in baseline data and the one in vitalsign data\n",
    "\n",
    "# Deal with categorical data. Transform catrgorical data to numerical data. \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_all['gender'] = le.fit_transform(df_all['gender'])\n",
    "correspondance_gender = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "df_all['heart_rhythm'] = le.fit_transform(df_all['heart_rhythm'])\n",
    "correspondance_heart_rhythm = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print(correspondance_gender)\n",
    "print(correspondance_heart_rhythm)\n",
    "\n",
    "df_all.to_csv('data/baseline_processed.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing for training of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the alive and dead patients\n",
    "To balance the number of training data for LSTM, we first seperate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# divide into part of alive patients and dead patients\n",
    "df = pd.read_csv('./data/vitalsign_processed.csv')\n",
    "dead_df = df[df['label_hosp'] == 1].reset_index().drop('index', axis=1)\n",
    "alive_df = df[df['label_hosp'] == 0].reset_index().drop('index', axis=1)\n",
    "\n",
    "\n",
    "dead_df = dead_df.drop('Unnamed: 0', axis=1)\n",
    "alive_df = alive_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Re-arrange the order of the dataframe\n",
    "new_order = list(alive_df.columns[0:3]) + list(alive_df.columns[4:6]) + list(alive_df.columns[3:4]) + list(alive_df.columns[6:])\n",
    "alive_df = alive_df[new_order]\n",
    "dead_df = dead_df[new_order]\n",
    "\n",
    "# save the result\n",
    "dead_df.to_csv('./data/dead_vitalsign.csv', index=False)\n",
    "alive_df.to_csv('./data/alive_vitalsign.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "alive_df = pd.read_csv('./data/alive_vitalsign.csv')\n",
    "dead_df = pd.read_csv('./data/dead_vitalsign.csv')\n",
    "\n",
    "# Sort DataFrame by patient_id and hour after the patient is admitted to the ICU\n",
    "alive_df = alive_df.sort_values(by=['stay_id', 'icu_timestep'])\n",
    "dead_df = dead_df.sort_values(by=['stay_id', 'icu_timestep'])\n",
    "\n",
    "# For patient with vitalsign records less than 24 hours, interpolate upto at least 24 hours\n",
    "def interpolate_up_to_24_rows(group):\n",
    "    num_to_interpolate = 24 - len(group)\n",
    "    min_hour = group['icu_timestep'].min()\n",
    "    max_hour = group['icu_timestep'].max()\n",
    "\n",
    "    interpolated_df = pd.DataFrame({'icu_timestep': np.random.uniform(min_hour, max_hour, num_to_interpolate)})\n",
    "    result_df = pd.merge(group, interpolated_df, on='icu_timestep', how='outer')\n",
    "    result_df = result_df.sort_values(by=['icu_timestep'])\n",
    "\n",
    "    result_df = result_df.interpolate(method='linear') # linear interpolate for the new generate data\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "# fill every stay_id to at least 24 timestep\n",
    "alive_df = alive_df.groupby('stay_id').apply(lambda group: interpolate_up_to_24_rows(group) if len(group) < 24 else group).reset_index(drop=True)\n",
    "dead_df = dead_df.groupby('stay_id').apply(lambda group: interpolate_up_to_24_rows(group) if len(group) < 24 else group).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add label to check whether dies in 24 hour and leave the icu\n",
    "alive_df['die_24hr'] = np.where((alive_df['icu_timestep_back'] <= 24) & (alive_df['label_icu'] == 1.0), 1, 0)\n",
    "dead_df['die_24hr'] = np.where((dead_df['icu_timestep_back'] <= 24) & (dead_df['label_icu'] == 1.0), 1, 0)\n",
    "\n",
    "\n",
    "# add label to check whether alive in 24 hour and leave the icu\n",
    "alive_df['alive_24hr'] = np.where((alive_df['icu_timestep_back'] <= 24) & (alive_df['label_icu'] == 0.0), 1, 0)\n",
    "dead_df['alive_24hr'] = np.where((dead_df['icu_timestep_back'] <= 24) & (dead_df['label_icu'] == 0.0), 1, 0)\n",
    "\n",
    "\n",
    "# modify the order of the column\n",
    "new_order = list(alive_df.columns[0:5]) + list(alive_df.columns[-2:]) + list(alive_df.columns[5:-2])\n",
    "alive_df = alive_df[new_order]\n",
    "dead_df = dead_df[new_order]\n",
    "\n",
    "# Set test label. If test label is true, only pick the first 24 hour for each patient\n",
    "test_label = True\n",
    "test_timestep_index = 0\n",
    "\n",
    "# Function to retrieve data in a sliding window\n",
    "def uniformly_sample_windows(group, window_size=24, sample_ratio=1):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    total_windows = len(group) - window_size + 1\n",
    "    sampled_windows = int(total_windows * sample_ratio)\n",
    "\n",
    "    if sampled_windows > 0:\n",
    "        indices = np.random.choice(total_windows, sampled_windows, replace=False)\n",
    "\n",
    "        if test_label:\n",
    "            test_timestep_index = len(group) - window_size\n",
    "            window = group.iloc[test_timestep_index:test_timestep_index + window_size, 7:]  # training data\n",
    "            label = group.iloc[test_timestep_index + window_size - 1, 0:7] # label\n",
    "            windows.append(window.values)\n",
    "            labels.append(label.values)\n",
    "        else:\n",
    "            for i in indices:\n",
    "                window = group.iloc[i:i + window_size, 7:]  # training data\n",
    "                label = group.iloc[i + window_size - 1, 0:7] # label\n",
    "                windows.append(window.values)\n",
    "                labels.append(label.values)\n",
    "\n",
    "    return np.array(windows), np.array(labels)\n",
    "\n",
    "# Apply the sliding window function to each patient group\n",
    "result_alive = alive_df.groupby('stay_id').apply(uniformly_sample_windows)\n",
    "result_dead = dead_df.groupby('stay_id').apply(uniformly_sample_windows)\n",
    "\n",
    "# Combine all sliding windows. \n",
    "data_alive_X, data_alive_y = [], []\n",
    "for X, y in result_alive.apply(lambda x: (x[0], x[1])):\n",
    "    if X.ndim == 3 and X.shape[1] == 24 and X.shape[2] == 12 and y.ndim == 2 and y.shape[1] == 7:\n",
    "        data_alive_X.append(X)\n",
    "        data_alive_y.append(y)\n",
    "data_dead_X, data_dead_y = [], []\n",
    "for X, y in result_dead.apply(lambda x: (x[0], x[1])):\n",
    "    if X.ndim == 3 and X.shape[1] == 24 and X.shape[2] == 12 and y.ndim == 2 and y.shape[1] == 7:\n",
    "        data_dead_X.append(X)\n",
    "        data_dead_y.append(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# save the concatenation of sliding window as .npy file\n",
    "if not test_label:\n",
    "\n",
    "    data_alive_X, data_alive_y = np.concatenate(data_alive_X, axis=0), np.concatenate(data_alive_y, axis=0)\n",
    "    data_dead_X, data_dead_y = np.concatenate(data_dead_X, axis=0), np.concatenate(data_dead_y, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    # random choose data from alive set\n",
    "    num_of_data = data_dead_X.shape[0]\n",
    "    selected_indice = np.random.choice(data_alive_X.shape[0], size=num_of_data, replace=False)\n",
    "    data_alive_X, data_alive_y = data_alive_X[selected_indice], data_alive_y[selected_indice]\n",
    "\n",
    "    data_alive_X_train, data_alive_X_test, data_alive_y_train, data_alive_y_test = train_test_split(data_alive_X, data_alive_y, test_size=0.2, random_state=42)\n",
    "    data_dead_X_train, data_dead_X_test, data_dead_y_train, data_dead_y_test = train_test_split(data_dead_X, data_dead_y, test_size=0.2, random_state=42)\n",
    "\n",
    "    data_X_train, data_X_test = np.concatenate((data_alive_X_train, data_dead_X_train), axis=0), np.concatenate((data_alive_X_test, data_dead_X_test), axis=0)\n",
    "    data_y_train, data_y_test = np.concatenate((data_alive_y_train, data_dead_y_train), axis=0), np.concatenate((data_alive_y_test, data_dead_y_test), axis=0)\n",
    "\n",
    "\n",
    "    np.save('./data_npy/data_X_train_new.npy', data_X_train)\n",
    "    np.save('./data_npy/data_y_train_new.npy', data_y_train)\n",
    "    np.save('./data_npy/data_X_test_new.npy', data_X_test)\n",
    "    np.save('./data_npy/data_y_test_new.npy', data_y_test)\n",
    "\n",
    "else:\n",
    "    data_alive_X, data_alive_y = np.concatenate(data_alive_X, axis=0), np.concatenate(data_alive_y, axis=0)\n",
    "    data_dead_X, data_dead_y = np.concatenate(data_dead_X, axis=0), np.concatenate(data_dead_y, axis=0)\n",
    "\n",
    "    data_X = np.concatenate((data_alive_X, data_dead_X), axis=0)\n",
    "    data_y = np.concatenate((data_alive_y, data_dead_y), axis=0)\n",
    "\n",
    "    # np.save('./data_npy/data_X_final.npy', data_X)\n",
    "    # np.save('./data_npy/data_y_final.npy', data_y)\n",
    "\n",
    "    np.save('./data_npy/data_X_final_least.npy', data_X)\n",
    "    np.save('./data_npy/data_y_final_least.npy', data_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data statistic for Level 1 & 2 baseline feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tableone import TableOne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ydata_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistic for level 1 baseline feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/baseline_level_1.csv')\n",
    "not_cols = ['subject_id', 'hadm_id', 'stay_id', 'intime']\n",
    "columns = [c for c in data.columns if c not in not_cols]\n",
    "categorical = ['label_hosp',  'gender', 'insurance', 'race', 'admission_type', 'label_icu'] # baseline1\n",
    "group_column = 'label_icu' # Target label. Label icu or Label hosp\n",
    "my_table = TableOne(data=data, columns=columns, categorical=categorical, pval=True, groupby=group_column)\n",
    "print(my_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistic for level 2 baseline feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/baseline_level_2.csv')\n",
    "not_cols = ['subject_id', 'hadm_id', 'stay_id', 'intime']\n",
    "columns = [c for c in data.columns if c not in not_cols]\n",
    "categorical = ['label_hosp', 'label_icu', 'sepsis3'] # baseline2\n",
    "group_column = 'label_icu' # Target label. Label icu or Label hosp\n",
    "my_table = TableOne(data=data, columns=columns, categorical=categorical, pval=True, groupby=group_column)\n",
    "print(my_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistic for Combined_comorbidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/Combined_comorbidity.csv')\n",
    "not_cols = ['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime', 'dod', 'subject_id_1', 'hadm_id_1', 'stay_id_1']\n",
    "columns = [c for c in data.columns if c not in not_cols]\n",
    "categorical = ['label_icu', 'label_hosp']\n",
    "group_column = 'label_icu' # Target label. Label icu or Label hosp\n",
    "my_table = TableOne(data=data, columns=columns, categorical=categorical, pval=True, groupby=group_column)\n",
    "print(my_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data statistic for Aspiii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/apsiii.csv')\n",
    "not_cols = ['subject_id', 'hadm_id', 'stay_id', 'intime', 'outtime', 'dod', 'subject_id_1', 'hadm_id_1', 'stay_id_1']\n",
    "columns = [c for c in data.columns if c not in not_cols]\n",
    "categorical = ['label_icu', 'label_hosp']\n",
    "group_column = 'label_icu' # Target label. Label icu or Label hosp\n",
    "my_table = TableOne(data=data, columns=columns, categorical=categorical, pval=True, groupby=group_column)\n",
    "print(my_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_hw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
